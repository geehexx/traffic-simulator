version: '3'

tasks:
  setup:
    desc: Setup Bazel build system
    cmds:
      - echo "Bazel build system is ready"

  test:
    desc: Run test suite with Bazel
    cmds:
      - bazel test //...

  run:
    desc: Run the simulator
    cmds:
      - bazel run //src/traffic_sim:traffic_sim_bin

  lint:
    desc: Run linting with Bazel
    cmds:
      - bazel build //...

  format:
    desc: Format code with Bazel
    cmds:
      - bazel build //...

  precommit:install:
    desc: Install pre-commit hooks for development assurance
    cmds:
      - uv run pre-commit install

  precommit:run:
    desc: Run pre-commit hooks manually
    cmds:
      - uv run pre-commit run --all-files

  quality:
    desc: Run quality analysis with Bazel
    cmds:
      - bazel build //...

  quality:monitor:
    desc: Run detailed quality monitoring with Bazel
    cmds:
      - bazel test //... --test_output=all

  quality:analyze:
    desc: Run comprehensive static analysis with Bazel
    cmds:
      - bazel query //...

  context:validate:
    desc: Validate context optimization compliance
    cmds:
      - uv run python scripts/validate_context_optimization.py

  context:optimize:
    desc: Run context optimization analysis and generate report
    cmds:
      - uv run python scripts/validate_context_optimization.py
      - echo "Context optimization analysis complete"

  context:report:
    desc: Generate context optimization report
    cmds:
      - uv run python scripts/validate_context_optimization.py
      - echo "Report generated in runs/quality/context_optimization_report.md"

  performance:
    desc: Run performance benchmark
    cmds:
      - bazel run //scripts:benchmarking_framework -- --mode=benchmark

  performance:scale:
    desc: Run scale performance testing
    cmds:
      - bazel run //scripts:benchmarking_framework -- --mode=scale

  performance:monitor:
    desc: Run real-time performance monitoring
    cmds:
      - bazel run //scripts:benchmarking_framework -- --mode=monitor

  profile:
    desc: Run simulation profiling
    cmds:
      - bazel run //scripts:benchmarking_framework -- --mode=profile

  validate:
    desc: Run validation tests
    cmds:
      - bazel test //tests:all_tests

  deps:check:
    desc: Check for unused dependencies with deptry
    cmds:
      - uv run deptry . --ignore DEP002,DEP003

  deps:unused:
    desc: Find unused imports with ruff
    cmds:
      - uv run ruff check src/ --select F401

  deps:audit:
    desc: Comprehensive dependency audit
    cmds:
      - echo "ðŸ” Checking for unused dependencies..."

  rules:validate:
    desc: Validate Cursor rules structure and compliance
    cmds:
      - python scripts/validate_rules.py
      - uv run deptry . --ignore DEP002,DEP003
      - echo "ðŸ” Checking for unused imports..."
      - uv run ruff check src/ --select F401
      - echo "âœ… Dependency audit completed"

  clean:
    desc: Clean all temporary files and caches from the project
    cmds:
      - bazel clean
      - find . -name "*.pyc" -delete
      - find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
      - find . -name ".pytest_cache" -type d -exec rm -rf {} + 2>/dev/null || true
      - find . -name ".hypothesis" -type d -exec rm -rf {} + 2>/dev/null || true
      - find . -name ".pyright_cache" -type d -exec rm -rf {} + 2>/dev/null || true
      - find . -name ".ruff_cache" -type d -exec rm -rf {} + 2>/dev/null || true
      - find . -name "*.log" -delete
      - find . -name ".coverage*" -delete
      - find . -name "htmlcov" -type d -exec rm -rf {} + 2>/dev/null || true
      - find . -name ".DS_Store" -delete
      - find . -name "*.tmp" -delete
      - find . -name "*.temp" -delete
      - echo "Cleaned all temporary files and caches"
