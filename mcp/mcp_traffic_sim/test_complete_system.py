#!/usr/bin/env python3
"""Test the complete prompt management system."""

from pathlib import Path
from prompt_registry import PromptRegistry
from meta_optimizer import MetaOptimizer
from continuous_improvement import ContinuousImprovementWorkflow


def test_complete_system():
    """Test the complete prompt management system."""

    print("ğŸš€ Testing Complete Prompt Management System")
    print("=" * 60)

    # Initialize system components
    registry_path = Path("mcp_registry")
    registry = PromptRegistry(registry_path)
    meta_optimizer = MetaOptimizer(registry)
    workflow = ContinuousImprovementWorkflow(registry)

    # Test 1: List registered prompts
    print("\nğŸ“‹ Testing Prompt Registry:")
    prompts = registry.list_prompts()
    print(f"âœ… Found {len(prompts)} registered prompts:")
    for prompt in prompts:
        print(f"  - {prompt.prompt_id}: {prompt.name}")

    # Test 2: Execute prompts
    print("\nğŸ¯ Testing Prompt Execution:")
    test_cases = [
        {
            "prompt_id": "generate_docs",
            "input": {
                "code_changes": "Implemented new collision detection system",
                "context": "Performance optimization for physics engine",
            },
        },
        {
            "prompt_id": "generate_rules",
            "input": {
                "patterns": "Collision detection patterns and performance",
                "context": "Physics engine optimization guidelines",
            },
        },
        {
            "prompt_id": "hybrid_maintenance",
            "input": {
                "mode": "hybrid",
                "task": "Comprehensive maintenance for collision system",
                "context": "Full documentation and rules update",
            },
        },
    ]

    for test_case in test_cases:
        print(f"\n  ğŸ§ª Testing {test_case['prompt_id']}:")
        result = registry.execute_prompt(test_case["prompt_id"], test_case["input"])
        print(f"    âœ… Success: {result.success}")
        print(f"    â±ï¸  Time: {result.execution_time:.3f}s")
        if result.success:
            print(f"    ğŸ“„ Output: {result.output['message'][:100]}...")
        else:
            print(f"    âŒ Error: {result.error_message}")

    # Test 3: Meta-optimization
    print("\nğŸ”§ Testing Meta-Optimization:")
    optimization_strategies = ["mipro", "bayesian", "hybrid"]

    for strategy in optimization_strategies:
        print(f"\n  ğŸ“Š Testing {strategy} optimization:")
        result = meta_optimizer.optimize_prompt("generate_docs", strategy)
        print(f"    âœ… Success: {result.success}")
        print(f"    ğŸ“ˆ Improvement: {result.improvement_score:.2f}")
        print(f"    â±ï¸  Time: {result.execution_time:.3f}s")
        if result.success:
            print(f"    ğŸ†• Optimized prompt: {result.optimized_prompt_id}")

    # Test 4: Continuous improvement workflow
    print("\nğŸ”„ Testing Continuous Improvement Workflow:")

    # Run optimization cycle
    cycle_results = workflow.run_optimization_cycle(
        ["generate_docs", "generate_rules"], ["mipro", "hybrid"]
    )
    print("âœ… Optimization cycle completed")
    print(f"ğŸ“Š Results: {len(cycle_results)} prompts optimized")

    # Test performance evaluation
    print("\nğŸ“Š Testing Performance Evaluation:")
    test_cases = [
        {"code_changes": "Test change 1", "context": "Test context 1"},
        {"code_changes": "Test change 2", "context": "Test context 2"},
        {"code_changes": "Test change 3", "context": "Test context 3"},
    ]

    performance = workflow.evaluate_prompt_performance("generate_docs", test_cases)
    print("âœ… Performance evaluation completed")
    print(f"ğŸ“ˆ Quality Score: {performance['overall_quality_score']:.2f}")
    print(f"â±ï¸  Average Time: {performance['average_execution_time']:.3f}s")

    # Test 5: Get improvement summary
    print("\nğŸ“‹ Testing Improvement Summary:")
    summary = workflow.get_improvement_summary()
    print("âœ… Summary generated")
    print(f"ğŸ”„ Total Cycles: {summary['total_cycles']}")
    print(f"ğŸ“Š Total Prompts: {summary['total_prompts_optimized']}")
    print(f"ğŸ“ˆ Average Improvement: {summary['average_improvement_score']:.2f}")

    print("\nğŸ‰ Complete system test successful!")
    print("\nğŸ“‹ System Capabilities Verified:")
    print("  âœ… Prompt registration and management")
    print("  âœ… Generic execute_prompt functionality")
    print("  âœ… Meta-optimization with multiple strategies")
    print("  âœ… Continuous improvement workflows")
    print("  âœ… Performance evaluation and metrics")
    print("  âœ… Optimization history tracking")

    return True


if __name__ == "__main__":
    test_complete_system()
