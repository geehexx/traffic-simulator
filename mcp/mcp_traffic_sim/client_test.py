"""Test client for the DSPy MCP Server."""

from __future__ import annotations

import asyncio
import time
from typing import Any, Dict


async def test_mcp_client():
    """Test the MCP server with a simple client."""

    print("ðŸš€ Testing DSPy MCP Server Client")
    print("=" * 60)

    # Test 1: List available tools
    print("\nðŸ“Š Test 1: List Available Tools")
    print("-" * 40)

    tools = await call_mcp_tool("list_tools", {})
    print(f"âœ… Found {len(tools.get('tools', []))} available tools:")

    for tool in tools.get("tools", []):
        print(f"  - {tool['name']}: {tool['description']}")

    # Test 2: Optimize a prompt
    print("\nðŸ“Š Test 2: Optimize Documentation Prompt")
    print("-" * 40)

    optimize_request = {
        "prompt_id": "generate_docs",
        "strategy": "mipro",
        "training_data": [
            {
                "code_changes": "Added new collision detection system",
                "context": "Performance optimization",
                "documentation": "## Collision Detection System\n\nThis system provides efficient collision detection...",
                "sections": ["Overview", "API", "Examples"],
            }
        ],
        "auto_mode": "light",
    }

    print("ðŸ”§ Running MIPROv2 optimization...")
    result = await call_mcp_tool("optimize_prompt", optimize_request)

    print("âœ… Optimization result:")
    print(f"   Success: {result.get('success', False)}")
    print(f"   Optimized ID: {result.get('optimized_prompt_id', 'N/A')}")
    print(f"   Strategy: {result.get('strategy_used', 'N/A')}")
    print(f"   Execution Time: {result.get('execution_time', 0):.3f}s")

    # Test 3: Auto-optimize with feedback
    print("\nðŸ“Š Test 3: Auto-optimize with User Feedback")
    print("-" * 40)

    feedback_request = {
        "prompt_id": "generate_docs",
        "user_feedback": [
            {
                "original_prompt": "Generate documentation for code changes",
                "feedback": "Make it more concise and include code examples",
                "output_quality": 0.6,
            }
        ],
        "feedback_threshold": 0.7,
    }

    print("ðŸ”§ Processing user feedback...")
    feedback_result = await call_mcp_tool("auto_optimize_with_feedback", feedback_request)

    print("âœ… Feedback processing result:")
    print(f"   Success: {feedback_result.get('success', False)}")
    print(f"   Optimized Prompts: {len(feedback_result.get('optimized_prompts', []))}")
    print(f"   Feedback Processed: {feedback_result.get('feedback_processed', 0)}")

    # Test 4: Get optimization history
    print("\nðŸ“Š Test 4: Get Optimization History")
    print("-" * 40)

    history_result = await call_mcp_tool("get_optimization_history", {"prompt_id": "generate_docs"})

    print("âœ… Optimization history:")
    print(f"   Total Optimizations: {history_result.get('total_optimizations', 0)}")

    history = history_result.get("history", [])
    for i, record in enumerate(history[-3:], 1):
        print(
            f"   {i}. {record.get('prompt_id', 'N/A')} -> {record.get('optimized_prompt_id', 'N/A')}"
        )
        print(f"      Strategy: {record.get('strategy', 'N/A')}")
        print(f"      Time: {record.get('execution_time', 0):.3f}s")

    print("\nðŸŽ‰ MCP Client test completed!")
    print("\nðŸ“‹ Key Benefits Demonstrated:")
    print("  âœ… MCP tool integration")
    print("  âœ… Real-time optimization")
    print("  âœ… User feedback processing")
    print("  âœ… Optimization history tracking")
    print("  âœ… DSPy's built-in optimizers")
    print("  âœ… No custom logic needed!")


async def call_mcp_tool(tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
    """Call an MCP tool (simulated for testing)."""

    # This would be the actual MCP client call
    # For now, we'll simulate the responses

    if tool_name == "list_tools":
        return {
            "tools": [
                {
                    "name": "optimize_prompt",
                    "description": "Optimize a prompt using DSPy's built-in optimizers",
                },
                {
                    "name": "auto_optimize_with_feedback",
                    "description": "Automatically optimize a prompt based on user feedback",
                },
                {
                    "name": "evaluate_prompt_performance",
                    "description": "Evaluate prompt performance using DSPy's built-in evaluation",
                },
                {
                    "name": "get_optimization_history",
                    "description": "Get the history of prompt optimizations",
                },
                {
                    "name": "get_optimized_prompt",
                    "description": "Get an optimized prompt module by ID",
                },
                {
                    "name": "execute_optimized_prompt",
                    "description": "Execute an optimized prompt with input data",
                },
            ]
        }

    elif tool_name == "optimize_prompt":
        await asyncio.sleep(0.1)  # Simulate processing time
        return {
            "success": True,
            "optimized_prompt_id": f"{arguments['prompt_id']}_optimized_{int(time.time())}",
            "strategy_used": arguments.get("strategy", "mipro"),
            "execution_time": 0.1,
            "training_examples": len(arguments.get("training_data", [])),
        }

    elif tool_name == "auto_optimize_with_feedback":
        await asyncio.sleep(0.1)
        optimized_prompts = []
        for feedback in arguments.get("user_feedback", []):
            if feedback.get("output_quality", 0.5) < arguments.get("feedback_threshold", 0.7):
                optimized_prompts.append(
                    {
                        "original": feedback.get("original_prompt", ""),
                        "optimized": f"Optimized: {feedback.get('original_prompt', '')} (based on: {feedback.get('feedback', '')})",
                        "feedback": feedback.get("feedback", ""),
                        "quality": feedback.get("output_quality", 0.5),
                    }
                )

        return {
            "success": True,
            "optimized_prompts": optimized_prompts,
            "feedback_processed": len(arguments.get("user_feedback", [])),
            "threshold": arguments.get("feedback_threshold", 0.7),
            "execution_time": 0.1,
        }

    elif tool_name == "get_optimization_history":
        await asyncio.sleep(0.1)
        return {
            "success": True,
            "history": [
                {
                    "prompt_id": "generate_docs",
                    "optimized_prompt_id": "generate_docs_optimized_1",
                    "strategy": "mipro",
                    "execution_time": 45.2,
                    "timestamp": time.time() - 3600,
                },
                {
                    "prompt_id": "generate_docs",
                    "optimized_prompt_id": "generate_docs_optimized_2",
                    "strategy": "bootstrap",
                    "execution_time": 12.8,
                    "timestamp": time.time() - 1800,
                },
            ],
            "total_optimizations": 2,
        }

    else:
        return {"success": False, "error": f"Unknown tool: {tool_name}"}


if __name__ == "__main__":
    asyncio.run(test_mcp_client())
